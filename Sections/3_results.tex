\section{Results}




The result of the fault injection for a single benchmark can be found in listing~\ref{lst:injection_results}. Outputs from the polybench benchmarks are stored as text files. Results from a benchmark run are stored as string sequences of decimals, separated by spaces, which are compared to the unmodified floating point implementation. The output seen in listing~\ref{lst:injection_results} is the average difference per value in the file between the fault injected version and the unmodified floating point version.  
The first column shows the filename where the output from the executable is stored. The filename contains which bit, counted from the right, has been flipped. The second column shows the average difference of each output compared to the equivalent output of the control file, which is the floating point enabled implementation. The final column displays the difference from the control of the alternative implementation, i.e., floating point implementation for the fixed point output and vice versa.

\begin{verbatim}[ label=lst:injection_results ]
seidel-2d.float.txt             0.0                       0.00010252590179347587
seidel-2d.txt                   0.00010252590179347587    0.0
seidel-2d_bit_no_1.fixed.txt    0.00010252973353766834    7.836499591601375e-15
seidel-2d_bit_no_1.float.txt    7.836499591601375e-15     0.00010252973353766834
seidel-2d_bit_no_2.fixed.txt    0.00010242978918456464    1.0070183402433041e-14
seidel-2d_bit_no_2.float.txt    1.0070183402433041e-14    0.00010242978918456464
seidel-2d_bit_no_3.fixed.txt    0.00010269591987037501    1.7122632695390493e-14
seidel-2d_bit_no_30.fixed.txt   240.38767306192983        8.453350257722744e-07
seidel-2d_bit_no_30.float.txt   8.453350257722744e-07     240.38767306192983
seidel-2d_bit_no_31.fixed.txt   432.92573090152945        2.020661384195993e-06
seidel-2d_bit_no_31.float.txt   2.020661384195993e-06     432.92573090152945
seidel-2d_bit_no_32.fixed.txt   0.00010252590179347587    6.188653891382238e-06
seidel-2d_bit_no_32.float.txt   6.188653891382238e-06     0.00010252590179347587
seidel-2d_bit_no_60.fixed.txt   0.00010252590179347587    1.1633619119162309e+79
seidel-2d_bit_no_60.float.txt   1.1633619119162309e+79    0.00010252590179347587
seidel-2d_bit_no_61.fixed.txt   0.00010252590179347587    1.3470810631989899e+156
seidel-2d_bit_no_61.float.txt   1.3470810631989899e+156   0.00010252590179347587
seidel-2d_bit_no_62.fixed.txt   0.00010252590179347587    nan
seidel-2d_bit_no_62.float.txt   nan                       0.00010252590179347587
seidel-2d_bit_no_63.fixed.txt   0.00010252590179347587    201.00625000002313
seidel-2d_bit_no_63.float.txt   201.00625000002313        0.00010252590179347587
\end{verbatim}



The control, seidel-2d.float.txt, is identical to itself, so the difference is 0.0. As the injected bit progresses toward the left, initially the fixed point difference is many orders of magnitude larger than the floating point difference, for instance for the 1st bit, the average difference between fixed point and control is larger by an order of magnitude of $10^{11}$ million than the difference between the floating point implementation. 

After bit number 54, the floating point average difference grows substantially from the previous bit injection, from 140.41412113413136 to 25619.825047812476, which is a factor of over 182. At this point, assuming that the floating point implementation follows the IEEE 754 standard, the bits flipped are in the exponent part of the floating point representation, which makes sense with the increase in size. there are many invalid results from bit no. 62 until the end, except for bit no. 63, which is likely the sign bit.

Furthermore there is no observed change in the fixed point output data compared to the original fixed point computation from injected bit no. 32 until injected bit no. 64, where the error multiplies with a factor of a million compared to injected bit no. 63.


Getting results required some elbow grease. Just getting \taffo{} to compile took a significant portion of time. The original aim of the project was to use both floatsmith and \taffo, and compare the two different methods of approximate computing, so work continued on floatsmith when stuck with taffo. Compiling the seidel benchmark with floatsmith resulted in errors, I only succeeded in building the demos included in the floatsmith docker image. 

When building \taffo{}  succeeded, the next goal was to use the existing framework provided in the benchmarks modified for use with \taffo{}  to perform fault injection. This required going into the code line by line, and refactoring certain parts, for instance the validation script used to compare the original floating point executable and the \taffo{} produced fixed point equivalent. 
Listing~\ref{lst:validate_original} shows one of the functions in the original script, and listing~\ref{lst:validate_new} shows the refactored version. The refactored version is functionally equivalent to the original and provides the same output, save for the headings in the output table being more descriptive.




%\begin{verbatim}[label=lst:validate_new, language=python]
\begin{lstlisting}[language=python,label=lst:validate_new,caption=Validate new]
def compute_difference(fixed_point_data, floating_point_data):
  successful_iterations = 0
  total_difference_between_floating_point_and_fixed_point_values = Decimal(0)
  total_floating_point_value = Decimal(0)
  fixed_point_invalid_result_count = 0
  floating_point_invalid_result_count = 0
  error_threshold = Decimal('0.01')

  for single_value_fixed_point, single_value_floating_point in zip(fixed_point_data, floating_point_data):
    fixed_point_value, floating_point_value = Decimal(single_value_fixed_point), Decimal(single_value_floating_point)


    if not fixed_point_value.is_finite():
      fixed_point_invalid_result_count += 1
    elif not floating_point_value.is_finite():
      floating_point_invalid_result_count += 1
      fixed_point_invalid_result_count += 1
    elif ((floating_point_value + fixed_point_value).copy_abs() - (floating_point_value.copy_abs() + fixed_point_value.copy_abs())) > error_threshold:
      fixed_point_invalid_result_count += 1
    else:
      successful_iterations += 1
      total_difference_between_floating_point_and_fixed_point_values += (floating_point_value - fixed_point_value).copy_abs()
      total_floating_point_value += floating_point_value
      
  average_error_percentage = (total_difference_between_floating_point_and_fixed_point_values / total_floating_point_value * 100) if total_floating_point_value != 0 and successful_iterations > 0 else -1
  average_absolute_error = (total_difference_between_floating_point_and_fixed_point_values / successful_iterations) if successful_iterations > 0 else -1
      
  return {'fixed_point_invalid_result_count': fixed_point_invalid_result_count if successful_iterations > 0 else "no successful iterations", \
          'floating_point_invalid_result_count': floating_point_invalid_result_count if successful_iterations > 0 else "no successful iterations",\
          'avg_percentage_error': str(average_error_percentage) + " %" if average_error_percentage != -1 else "no successful iterations",
          'avg_absolute_error': average_absolute_error if average_absolute_error != -1 else "no successful iterations"}
\end{lstlisting} 
%\end{verbatim}


%\begin{verbatim}[label=lst:validate_original, language=python]
\begin{lstlisting}[label=lst:validate_original,caption=Validate original,language=python]
def ComputeDifference(fix_data, flt_data):
  n = 0
  accerr = Decimal(0)
  accval = Decimal(0)
  fix_nofl = 0
  flo_nofl = 0

  thres_ofl_cp = Decimal('0.01')

  for svfix, svflo in zip(fix_data, flt_data):
    vfix, vflo = Decimal(svfix), Decimal(svflo)

    if not vfix.is_finite():
      fix_nofl += 1
    elif not vflo.is_finite():
      flo_nofl += 1
      fix_nofl += 1
    elif ((vflo + vfix).copy_abs() - (vflo.copy_abs() + vfix.copy_abs())) > thres_ofl_cp:
      fix_nofl += 1
    else:
      n += 1
      accerr += (vflo - vfix).copy_abs()
      accval += vflo
      
  e_perc = (accerr / accval * 100) if accval != 0 and n > 0 else -1
  e_abs = (accerr / n) if n > 0 else -1
      
  return {'fix_nofl': fix_nofl, \
          'flo_nofl': flo_nofl, \
          'e_perc': e_perc,
          'e_abs': e_abs}
\end{lstlisting}
%\end{verbatim}

While the function in listing~\ref{lst:validate_new} may seem unnecessarily verbose, it is still difficult to understand some parts of the code.

The comparison script written for the fault injection part was written in a separate repository with tests, and the script was copied over iteratively after adding new features (and tests).

The new compilation and run shell scripts were heavily influenced by the existing compile.sh and run.sh scripts, but edited for the purposes of this project. The new run script provided functionality to run multiple files of the same benchmark with bitwise faults injected at different locations, and the new compile script likewise allows the user to easily compile multiple versions of the same script with bitwise faults at different locations.


\subsection{documenting deviations}

There have been some omissions and deviations which are not documented in neither the code repository nor the papers which can be submitted as a pull request to the \taffo project. 
This includes facts such as the error propagator not actually doing anything, and the installation instructions of \taffo not being foolproof (though the bigger the fool, the more difficult to proof).

Though there is both a test and a test-lit folder in the taffo project, neither is documented in the readme or elsewhere, and the tests themselves are cryptic at best. While one would have to assume some knowledge of a testing framework, the tests do not declare what they test. This holds for both test folders. 

In addition to not being very declarative in the test descriptions, the tests folder contains a lot of benchmarks. These are not tests in that they do not assert some kind of functionality that can be verified, only whether the code produces \*some\* output. Therefore it should not reside in the tests folder, but perhaps a folder named benchmarks.


