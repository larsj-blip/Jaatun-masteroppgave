
The results of the thesis work consist of a fault injection campaign on a benchmark included in the \taffo{} repository and code additions. These additions reside in a clone of the \taffo{} repository hosted on \href{https://github.com/larsj-blip/TAFFO}{https://github.com/larsj-blip/TAFFO}. The results include the experiences gathered from using \taffo{} and floatsmith to compile a test benchmark.


\section{Taffo}
The most significant results of the fault injection for a single benchmark can be found in table~\ref{table:injection_results}. The complete output can be found in appendix~\ref{appendix:complete_injection_results}. Outputs from the polybench benchmarks are stored in text files as string sequences of decimals separated by spaces. Taking the outputs from the floating point without injected faults as the control, the element by element difference between the control and the fault injected program versions is added up, and divided by the amount of data points to get the average difference for all output values. Table~\ref{table:injection_results} shows the average difference per data point between the output files of the fault injected versions and the unmodified floating point version.   
The first column shows the filename where the output from the executable is stored. The filename is built from the benchmark name, which in this case is seidel-2d. The next part of file name contains the location of the injected bit in the variable, with "bit\_no\_0" representing a bit flip in the least significant bit of the variable (at index 0). The final part of the filename specifies whether the file output stems from a floating point implementation or a fixed point implementation.
The first fixed point implementation of the benchmark without any faults injected is an exception, this is the entry with filename seidel-2d.txt.

The second column shows the result of the above mentioned comparison between the unmodified floating point implementation of the benchmark, seidel-2d, and the implementation specified in the leftmost column. 

%  DETTE VAR REPETISJON The value is calculated by taking the absolute value of the difference of each value at index [x] for all values of x, and dividing the accumulated difference by the amount of values. This gives the average difference per data point between the two outputs.

The values in column three are redundant and are present only for consistency with the actual output of the comparison script. For a given fault injection location, column three shows the comparison results for the alternative implementation of the program. For example, the value in column three for the entry seidel-2d\_bit\_no\_0.fixed.txt is the same as the value in column two in the entry at seidel-2d\_bit\_no\_0.float.txt. This was done because this was quicker than implementing sorting for the print function in the comparison script. The outputs in listing~\ref{table:injection_results} and in appendix~\ref{appendix:complete_injection_results} are ordered manually.


%\begin{longtable}{lll}
\begin{table}
\caption{The table shows the output of the simple python script that collects the absolute differences between the original floating point implementation and the fault injected versions of the benchmark.}
\label{table:injection_results}
\begin{tabular}{lll}
   

{\bf Filename} & {\bf Average Difference} & {\bf Alternative } \\
       ~      & {\bf From Control }      & {\bf Implementation Results}\\
 \hline
    seidel-2d.float.txt & 0.0   & 0.00010252590179347587 \\
    seidel-2d.txt & 0.00010252590179347587 & 0.0 \\
    seidel-2d\_bit\_no\_0.fixed.txt & 0.00010252738737963263 & 4.526846432428844e-15  \\
    seidel-2d\_bit\_no\_0.float.txt &  4.526846432428844e-15 & 0.00010252738737963263 \\
    seidel-2d\_bit\_no\_1.fixed.txt & 0.00010252973353766834 & 7.836499591601375e-15 \\
    seidel-2d\_bit\_no\_1.float.txt & 7.836499591601375e-15 & 0.00010252973353766834 \\
    seidel-2d\_bit\_no\_2.fixed.txt & 0.00010242978918456464 & 1.0070183402433041e-14 \\
    seidel-2d\_bit\_no\_2.float.txt & 1.0070183402433041e-14 & 0.00010242978918456464 \\
    seidel-2d\_bit\_no\_3.fixed.txt & 0.00010269591987037501 & 1.7122632695390493e-14 \\
    seidel-2d\_bit\_no\_3.float.txt & 1.7122632695390493e-14 & 0.00010269591987037501 \\
    seidel-2d\_bit\_no\_30.fixed.txt & 240.38767306192983 & 8.453350257722744e-07 \\
    seidel-2d\_bit\_no\_30.float.txt & 8.453350257722744e-07 & 240.38767306192983 \\
    seidel-2d\_bit\_no\_31.fixed.txt & 432.92573090152945 & 2.020661384195993e-06 \\
    seidel-2d\_bit\_no\_31.float.txt & 2.020661384195993e-06 & 432.92573090152945 \\
    seidel-2d\_bit\_no\_32.fixed.txt & 0.00010252590179347587 & 6.188653891382238e-06 \\
    seidel-2d\_bit\_no\_32.float.txt & 6.188653891382238e-06 & 0.00010252590179347587 \\
    seidel-2d\_bit\_no\_60.fixed.txt & 0.00010252590179347587 & 1.1633619119162309e+79 \\
    seidel-2d\_bit\_no\_60.float.txt & 1.1633619119162309e+79 & 0.00010252590179347587 \\
    seidel-2d\_bit\_no\_61.fixed.txt & 0.00010252590179347587 & 1.3470810631989899e+156 \\
    seidel-2d\_bit\_no\_61.float.txt & 1.3470810631989899e+156 & 0.00010252590179347587 \\
    seidel-2d\_bit\_no\_62.fixed.txt & 0.00010252590179347587 & nan \\
    seidel-2d\_bit\_no\_62.float.txt & nan   & 0.00010252590179347587 \\
    seidel-2d\_bit\_no\_63.fixed.txt & 0.00010252590179347587 & 201.00625000002313 \\
    seidel-2d\_bit\_no\_63.float.txt & 201.00625000002313 & 0.00010252590179347587 \\
    \end{tabular}%
%  \label{tab:addlabel}%
%\end{longtable}%
\end{table}


The control, seidel-2d.float.txt, is identical to itself, so the difference is 0.0. Initially the fixed point difference is many orders of magnitude larger than the floating point difference, for the result from injected bit at index 0, the average difference between fixed point and control is larger by an order of magnitude of 11 compared to the difference between the floating point implementation. 

After bit number 54, the floating point average difference grows substantially from the previous bit injection, from 140.41412113413136 to 25619.825047812476, which is a factor of over 182. At this point, assuming that the floating point implementation follows the IEEE 754 standard, the bits flipped are in the exponent part of the floating point representation, which makes sense with the increase in size. The result for injected bit at index 62 is undefined for the floating point implementation, due to many NaN (not a number) outputs. There is no observed change in the fixed point output data compared to the original fixed point computation from injected bit at index 32 until injected bit at index 63.


When building \taffo{}  succeeded, the next goal was to use the existing framework provided in the benchmarks modified for use with \taffo{} to perform fault injection. There is an existing validate.py python script used to compare the standard floating point implementations of the polybench benchmarks and \taffo{}-produced fixed point implementations, and this was used to gain a basic understanding of how the outputs of the benchmarks were used to evaluate the performance and error of the \taffo{} code transformations. Line by line the code was refactored to be more verbose, and to more clearly represent the actions happening in the script.
Listing~\ref{lst:validate_original} shows one of the functions in the original script, and listing~\ref{lst:validate_new} shows the refactored version. The refactored version is functionally equivalent to the original and provides the same output, save for modified dictionary keys in the output. This is because the script uses the dictionary keys as headings. 




\begin{lstlisting}[language=python,label=lst:validate_new,caption=Validate new]
def compute_difference(fixed_point_data, floating_point_data):
    successful_iterations = 0
    total_difference_floating_and_fixed_point_values = Decimal(0)
    total_floating_point_value = Decimal(0)
    fixed_point_invalid_result_count = 0
    floating_point_invalid_result_count = 0
    error_threshold = Decimal('0.01')

    for single_value_fixed_point, single_value_floating_point in zip(fixed_point_data, floating_point_data):
        fixed_point_value, floating_point_value = Decimal(single_value_fixed_point), Decimal(
            single_value_floating_point)

        if not fixed_point_value.is_finite():
            fixed_point_invalid_result_count += 1
        elif not floating_point_value.is_finite():
            floating_point_invalid_result_count += 1
            fixed_point_invalid_result_count += 1
        elif ((floating_point_value + fixed_point_value).copy_abs() - (
                floating_point_value.copy_abs() + fixed_point_value.copy_abs())) > error_threshold:
            fixed_point_invalid_result_count += 1
        else:
            successful_iterations += 1
            total_difference_floating_and_fixed_point_values += (
                        floating_point_value - fixed_point_value).copy_abs()
            total_floating_point_value += floating_point_value

    average_error_percentage = (
                total_difference_floating_and_fixed_point_values / total_floating_point_value * 100) \
        if total_floating_point_value != 0 and successful_iterations > 0 else -1
    average_absolute_error = (
                total_difference_floating_and_fixed_point_values / successful_iterations) \
        if successful_iterations > 0 else -1

    return {
        'fixed_point_invalid_result_count': 
            fixed_point_invalid_result_count if successful_iterations > 0 else "no successful iterations",
        'floating_point_invalid_result_count': 
            floating_point_invalid_result_count if successful_iterations > 0 else "no successful iterations",
        'avg_percentage_error': 
            str(
            average_error_percentage) + " %" if average_error_percentage != -1 else "no successful iterations",
        'avg_absolute_error': average_absolute_error if average_absolute_error != -1 else "no successful iterations"}

\end{lstlisting} 


\begin{lstlisting}[label=lst:validate_original,caption=Validate original,language=python]
def ComputeDifference(fix_data, flt_data):
  n = 0
  accerr = Decimal(0)
  accval = Decimal(0)
  fix_nofl = 0
  flo_nofl = 0

  thres_ofl_cp = Decimal('0.01')

  for svfix, svflo in zip(fix_data, flt_data):
    vfix, vflo = Decimal(svfix), Decimal(svflo)

    if not vfix.is_finite():
      fix_nofl += 1
    elif not vflo.is_finite():
      flo_nofl += 1
      fix_nofl += 1
    elif ((vflo + vfix).copy_abs() - (vflo.copy_abs() + vfix.copy_abs())) > thres_ofl_cp:
      fix_nofl += 1
    else:
      n += 1
      accerr += (vflo - vfix).copy_abs()
      accval += vflo
      
  e_perc = (accerr / accval * 100) if accval != 0 and n > 0 else -1
  e_abs = (accerr / n) if n > 0 else -1
      
  return {'fix_nofl': fix_nofl, \
          'flo_nofl': flo_nofl, \
          'e_perc': e_perc,
          'e_abs': e_abs}
\end{lstlisting}




The comparison python script that was used to create the results in listing~\ref{lst:injection_results} was written in a separate github repository, and transplanted over to the \taffo{} repository whenever it was updated. This was done out of percieved convenience, but the directory with the script and the tests might just as well have been located in the taffo repository. In hindsight, this is probably also easier.

The comparison script was written with tests documenting the behavior, and comments pointing out missing tests and undocumented behavior. Especially the tests are vital for external programmers to understand the intent behind the code, and more tests would have made it easier to understand the existing behavior in the \taffo{} repository. 

The edited scripts for compilatoins and running the benchmarks were heavily influenced by the existing compile.sh and run.sh scripts located in the tests/polybench-cpu directory, but edited for the purposes of this thesis. The new script for running the benchmarks provided functionality to run multiple files of the same benchmark with bitwise faults injected at different locations, and the new compile script likewise allows the user to easily compile multiple versions of the same script with bitwise faults at different locations. Instructions on the use of these scripts to repeat/extend the experiment is contained within the scripts themselves.


\section{Floatsmith}

Floatsmith was selected as an option for fault injection campaigns along with \taffo{} because of source code being available, and the program being installable through a docker image. Initial studies of floatsmith were promising, as transforming the code included in the demos was easy. 
However, when attempting to apply floatsmith to the benchmark code located in \taffo{} the result was an error, and no hints as to what may have been the problem. In the paper detailing floatsmith, the authors state that the tool has not been tested on larger codebases, and this may be a part of the problem. The demo scripts that comes with the floatsmith installation are single file programs, while the polybench benchmarks that come with \taffo{} are all multi-file programs. This, together with outdated/lacking documentation (for instance, at the moment there are dead links in the repository README), lead to no numerical results, but this in itself is a result.


