\section{Introduction}

One of the enduring challenges of computing is reducing power requirements of computers while also increasing processing power. For many years, decreasing transistor size while increasing the amount of transistors was the best bet, and this evolved into multi-core processing that we have today. While the largest leaps have been made on the hardware side, improvements seem to be reaching a plateau. This has encouraged efforts to improve resource usage through software. One such effort is approximate computing. 

\emph{Approximate computing}, at its core, aims to gain either reduced power consumption, higher processing speeds, or both at the same time, at the cost of the precision of the results. This reduction in precision is implemented either as less accurate algorithms that are less resource intensive but not guaranteed to get an accurate result. This can be likened to a greedy graph traversal algorithm, such as the greedy best-first search~\citep{coles2007marvin}, that finds a path between two nodes in a graph quickly, but is not guaranteed to find the shortest/optimal path, instead of an algorithm that is guaranteed to find the shortest path, such as Dijkstra's algorithm~\citep{dijkstra1959note}, that is slower to run.

%Critical infrastructure and other critical fields can benefit from approximate computing.
Some fields that can benefit from approximate computing are scientific simulations and IoT-based critical infrastructure components. In simulation and high precision data processing, approximate computing may reduce both time spent on calculations and power consumed at the cost of precision. Weather simulations for use in weather reports are heavy calculations that have to be re-done often due to rapidly changing parameters that change the results, and might therefore benefit from reducing precision and instead increasing the amount of times the simulations are run with updated parameters. Critical infrastructure installations such as hydro power plants or water reservoirs depend more on and more on IoT-devices for monitoring. These monitoring nodes are largely comprised of low-powered hardware powered by solar or batteries. Implementing approximate computing for IoT-nodes running on low-power systems may increase longevity of these nodes. Image processing and recognition done in modern cars for obstacle detection and avoidance may also benefit from lower power draw and increased processing speed for quicker response to events and (especially for EVs) lower power consumption leading to longer range. %litt søkt kanskje? kronglete setning. Siter Water-Tight IoT–Just Add Security

To augment applications that require correct service, a more thorough analysis of how approximate computing techniques affect the dependability of an application is required.

Fault tolerance is in simple terms a measure of how well a service can be provided by a provider in the presence of faults. A more accurate description is provided in section~\ref{section:Reliability_thorugh_fault_tolerance}. Fault tolerance is vital for applications in critical infrastructure such as power plants, nuclear or otherwise, dams, power grid installations, and real time systems with strict operating requirements, such as self driving cars or aviation equipment. In the worst case, a fault that becomes an error in these fields may result in the loss of life. 

Code quality is important to achieve fault tolerant code, and should be evaluated alongside the operational fault tolerance of a software tool. While the operational aspect is certainly important, i.e., how the code responds to faults that occur during operation, a projects code quality will affect the amount of faults that are created during the developmental phase, which may result in service failures during operation. 

There are multiple papers on the investigation of fault tolerance through hardware fault injection, see section~\ref{section:Verifying_fault_tolerance} for a more thorough overview. Hardware fault injection can be performed through ionizing radiation, heat or applying local voltages to the hardware. Fault tolerance experiments such as these are performed mostly to evaluate hardware that will be subjected to harsh conditions. 
Fault injection can also be performed in software. For instance, \todo[inline]{(PAPER ON GEM-V FAULT INJECTION HERE)} flips individual bits in the program, and simulates the running of these programs to find all bits in a program that are conducive to silent data corruptions.  G-SWFIT is a methodology of injecting software faults that would otherwise fall outside of what a programmer could speed from computers catch during testing. 

%These strategies have not been applied to approximate computing as a whole, so as to enable discussions of fault tolerance in approximate computing.

As far as an initial survey of available papers indicates, there is no larger body of research on the fault tolerance properties that approximate computing-enabled code provides. The aim of this thesis is to create a foundation that may elicit further work in the space.